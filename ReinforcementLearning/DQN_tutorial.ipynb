{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to use Pytorch to train a Deep Q Learning(DQN) agent on the `CartPole-v0` task from the OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:53.695695Z",
     "start_time": "2019-06-12T19:09:53.337941Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:53.915151Z",
     "start_time": "2019-06-12T19:09:53.697800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:54.181767Z",
     "start_time": "2019-06-12T19:09:53.918311Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qianqu/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").unwrapped\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay will help us to handle two things:\n",
    "* Avoid forgetting previous experiences.\n",
    "* Reduce correlations between experiences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to classes:\n",
    "\n",
    "* `Transition` - a tuple representing a single transition in our environment. It maps (state, action) pairs to their (next_state, reward)\n",
    "* `ReplayMemory` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implement a `.sample()` method for selecting a random batch of transitions for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:54.187051Z",
     "start_time": "2019-06-12T19:09:54.183588Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                       ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:54.195334Z",
     "start_time": "2019-06-12T19:09:54.189117Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        \n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position+1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Q-network would be a convolutional neural network that takes in the difference between the current and previous screen patches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:54.205196Z",
     "start_time": "2019-06-12T19:09:54.197171Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3,16,kernel_size=5,stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size=5,stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32,32,kernel_size=5,stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        def conv2d_size_out(size, kernel_size=5, stride=2):\n",
    "            # calculate the output size of convolutional layer\n",
    "            return (size - kernel_size) // stride + 1\n",
    "        \n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_size = convw * convh * 32\n",
    "        \n",
    "        self.head = nn.Linear(linear_size, outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        out = self.head(x.view(x.size(0),-1)) # x.size(0) is batch size\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It use `torchvision.transform` to compose image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:11:28.210294Z",
     "start_time": "2019-06-12T19:11:27.977606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_cart_location:  594\n",
      "scale 250.0\n",
      "screen_width 1200\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADECAYAAACP3tqSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEt1JREFUeJzt3X2UXHV9x/H3J5tnCHmQBUISDFCqIJWgEYJYC8hDpFU4Pdpjamvs4TQ99bkHRbCthR7aQg8KtXq0lCCIFEFAoKmt0vBg6QOw4TEQMZhAEojJIkQQImF3v/3j/jaZGXZ3Zndn505++bzOuWfv794793723rvfvfObmTuKCMzMbPc3ruwAZmbWHC7oZmaZcEE3M8uEC7qZWSZc0M3MMuGCbmaWCRd0azlJ50v6dtk52omkEyRtKjuH7d5c0G2PIOmjku4Zw/VfJenCsVq/WSNc0M0SSR1lZxgrksaXncHGngt6ZiQdKOkmSd2S1kv6VMW870v6UkX7eklXpvFDJd0h6eeSnpN0raQZFcs+Jelzkh6R9LKk5ZL2l/Tvkl6S9J+SZqZl50sKScskPStps6Szh8i8SNL/SNom6WFJJzTz95N0OPAN4DhJv5S0Lc2/StLX0+NeBk6U9NuSHpT0oqSNks6v2f67KrJuTFf+y4APA+ek9f9rA1mnpO2/IOlx4B1D/M6SdKmkrZJ+kY7BkRXr+ZKkp9O8e9K0/mNwlqQNwB319rWk6em4bpb0jKQL+//J9T/DkXRJyrxe0nsHy2wliQgPmQwU/6BXAV8EJgKHAOuA09L8A4CtwEkUBWgdMC3N+zXgFGAS0An8CLisYt1PAf8H7A/MSet5ADg6PeYO4K/SsvOBAK4D9gJ+A+gGTk7zzwe+ncbnAD8HTk/5T0ntzib/fh8F7qlZ31XAL4Dj07onAyekvOOAtwJbgDPT8gcBLwFLgAnAG4AFFeu6cBhZLwL+C5gFzANWA5sGOa6npXXNAAQcDsxO874G3JX2YwfwznQ8+o/Bt9IxmFJvXwO3AP+Ult8PuA/4k4r99xrwx2k7fwo8C6js895DxblSdgAPTTyYcCywoWbaecA3K9q/C2wEngPeNcS6zgQerGg/BXy4on0T8PWK9ieBW9J4fzF5c8X8vweWp/Hz2VXQPw9cU7PtHwBLm/n7MXhB/1adfXoZcGnFtr43yHJXUV3Qh8xKUdwXV8xbNkRBPwn4CbAIGFcxfRywHThqgMf0H4NDKqYNuq8p/lG/CkypmLcEuLNi/z1ZMW9qWv8BZZ/3HnYN7lfLyxuBA/u7FJIOiivBfiuArwJPRMTOFwkl7Qd8BfhNYBpFsXihZv1bKsa3D9Deu2b5jRXjT1Nc+Q6U+YOS3lcxbQJw5yDLjuj3G0JlRiQdS3H1fCTFlfUk4Ltp9jzgpw2ss5GsB/L6/TOgiLhD0lcprsYPkvQ94LMUzygm18lUuY2h9vUb0/hmSf3zxtU8/mcVmV5Jy9UecyuR+9DzshFYHxEzKoZpEXF6xTJ/A6wBZktaUjH97yiuuN4aEfsAf0Dx9H405lWMH0TxFH2gzNfUZN4rIi4aZNmR/n6D3Va0dvq/ALcB8yJiOkXfe/9+2Agc2uB66mXdzOv3z6Ai4isR8XbgLcCvA5+jeBbyqyEy1eYaal9vpLhC37di3j4R8Zahcll7cUHPy33Ai5I+n14Y65B0pKR3AEh6N/BHwEfS8I+S5qTHTgN+CWxL0z7XhDx/KWmqpLek7V4/wDLfBt4n6bSUd7KK92TPbfLvtwWYK2linczTgOcj4leSjgF+v2LetcDJkn5P0nhJb5C0oGL9hzSaFbgBOE/SzPS7fnKwQJLeIelYSROAlymKeG9E9AFXAl9OL8B2SDpO0qRBVjXovo6IzcAPgS9J2kfSOBUvlP9Wnf1lbcQFPSMR0Qu8D1gArKe4grsCmC5pH4oXyD4REc+k7ojlwDdVPHe+AHgbxYuE/wbc3IRIdwNPAiuBSyLihwNk3gicAXyB4oXTjRT/TF53bo7y97sDeAz4maTnhsj8MeCvJb1E8YLmDRXb30DxguLZwPPAQ8BRafZy4Ij07pFbhsqalr+AoptlPUUhvWaITPsA/0zRBfY0xQuZl6R5nwUeBe5PmS4eaN+l/PX29UcoupkeT9u6EZg9RC5rM4rwF1xYc0maT1GoJkRET7lpzPYcvkI3M8uEC7qZWSbc5WJmlolRXaFLWizpCUlPSjq3WaHMzGz4RnyFnu7x8BOKjw9voniVfUlEPN68eGZm1qjRfFL0GIqPAq8DkPQdirdEDVrQ991335g/f/4oNmlmtudZtWrVcxHRWW+50RT0OVR/LHgTxf0rBjV//ny6urpGsUkzsz2PpEFvDVFpNH3oA30s/HX9Nypuodolqau7u3sUmzMzs6GMpqBvovpeFHMZ4F4dEXF5RCyMiIWdnXWfMZiZ2QiNpqDfDxwm6eB0f4wPUdzUyMzMSjDiPvSI6JH0CYr7KXcAV0bEY01LZmZmwzKq+6FHxPeB7zcpi5mZjYK/4KKO6OurbA25rMZl+x3DZrYb8L1czMwy4YJuZpYJF3Qzs0y4D72OF9bdv3P8mftuqZo3ftJeVe1DT/tYVXvi3rPGLpiZWQ1foZuZZcIF3cwsEy7oZmaZcB96HdG76zuOtz+/qWpex8SpVe2+nl+1JJOZ2UB8hW5mlgkXdDOzTLigm5llwn3o9WjX93jU3qvl9fduGeg7P8zMWsNX6GZmmXBBNzPLhAu6mVkmXNDNzDLhgm5mlgkXdDOzTLigm5llwgXdzCwTLuhmZplwQTczy4QLuplZJlzQzcwy4YJuZpYJF3Qzs0y4oJuZZaJuQZd0paStklZXTJsl6XZJa9PPmWMb08zM6mnkCv0qYHHNtHOBlRFxGLAytc3MrER1v7EoIn4kaX7N5DOAE9L41cBdwOebmKttjJ88bee4VP0NRX29r1W1e7a/VP3gGbPHLJeZWa2R9qHvHxGbAdLP/ZoXyczMRmLMXxSVtExSl6Su7u7usd6cmdkea6QFfYuk2QDp59bBFoyIyyNiYUQs7OzsHOHmzMysnpEW9NuApWl8KXBrc+K0n3EdE3cOSNVDRNUQfX1Vg5lZKzXytsXrgP8F3iRpk6SzgIuAUyStBU5JbTMzK1Ej73JZMsis9zQ5i5mZjYI/KWpmlom6V+gWZQcwM2uIr9DNzDLhgm5mlgkXdDOzTLigm5llwgXdzCwTLuhmZplwQTczy4QLuplZJlzQzcwy4YJuZpYJF3Qzs0y4oJuZZcIF3cwsEy7oZmaZ8O1z65EaXzZ8q10zK4+v0M3MMuGCbmaWCXe51DFhyrSd4+PGT6ya17vjlar2ju3bWpLJzGwgvkI3M8uEC7qZWSZc0M3MMuE+9Hr8tkUz2034Ct3MLBMu6GZmmXBBNzPLhAu6mVkm6hZ0SfMk3SlpjaTHJH06TZ8l6XZJa9PPmWMf18zMBtPIFXoPcHZEHA4sAj4u6QjgXGBlRBwGrExtMzMrSd2CHhGbI+KBNP4SsAaYA5wBXJ0Wuxo4c6xCmplZfcPqQ5c0HzgauBfYPyI2Q1H0gf2aHc7MzBrXcEGXtDdwE/CZiHhxGI9bJqlLUld3d/dIMpqZWQMaKuiSJlAU82sj4uY0eYuk2Wn+bGDrQI+NiMsjYmFELOzs7GxGZjMzG0Aj73IRsBxYExFfrph1G7A0jS8Fbm1+PDMza1Qj93I5HvhD4FFJD6VpXwAuAm6QdBawAfjg2EQ0M7NG1C3oEXEPMNgdqt7T3DhmZjZS/qSomVkmfPvcphrGrXbNzJrMV+hmZplwQTczy4QLuplZJtyHXofGdVSM1/z/q/nGub7eHS1IZGY2MF+hm5llwgXdzCwT7nKpo2Pi1J3j48ZPqppX3E14l55XqttmZq3kK3Qzs0y4oJuZZcIF3cwsEy7oZmaZcEE3M8uEC7qZWSZc0M3MMuGCbmaWCRd0M7NMuKCbmWXCBd3MLBMu6GZmmXBBNzPLhAu6mVkmXNDNzDLhgm5mlgkXdDOzTLigm5llwl9B10xS2QnMbA/mK3Qzs0zULeiSJku6T9LDkh6TdEGafrCkeyWtlXS9pIljH9fMzAbTyBX6q8BJEXEUsABYLGkRcDFwaUQcBrwAnDV2Mc3MrJ66BT0Kv0zNCWkI4CTgxjT9auDMMUlYsvGTpuwcOiZMqhpEb9XQt2N71WBm1koN9aFL6pD0ELAVuB34KbAtInrSIpuAOYM8dpmkLkld3d3dzchsZmYDaKigR0RvRCwA5gLHAIcPtNggj708IhZGxMLOzs6RJzUzsyEN622LEbFN0l3AImCGpPHpKn0u8OwY5BuRnp6eqvayZct2jm/YsGFY6+qo+Jf3qVMPrpq334wDq9orbr2xqv3dv/jGsLbV79RTT61qn3POOSNaj5ntWRp5l0unpBlpfApwMrAGuBP4QFpsKXDrWIU0M7P6GrlCnw1cLamD4h/ADRGxQtLjwHckXQg8CCwfw5xmZlZH3YIeEY8ARw8wfR1Ff7qZmbWBLD/6H1H9+uzdd9+9c3zdunUjXu/CI75W1Z4V765qP7L2K1XtlSuvH9F2DjjggBE9zsz2bP7ov5lZJlzQzcwy4YJuZpaJLPvQx42r/j81adKkEa9rwvhd6+rtmFU1Tx3Tqtp9NfNHauJE3+fMzIbPV+hmZplwQTczy4QLuplZJlrah97T00Mr7rhYey+X3t7eEa+rt7dv5/hdt/9t1TxNPqiq/fLzD494O5W2b6++9a7vUmlmjfAVuplZJlzQzcwy0dIuF0kteUuepCHbw9FXcReBex99tGZubbs5at926bcxmlkjfIVuZpYJF3Qzs0y4oJuZZaKlfegdHR1Mnz59zLdT+zbF2j7pdld7q4JW7DMz2/3tXpXOzMwG5YJuZpYJF3Qzs0xkefvcvr6+qvarr75aUpKR2bFjR9kRzGw35Ct0M7NMuKCbmWXCBd3MLBNZ9qHX3rvlxBNP3Dl+6KGHtjrOsC1YsKDsCGa2G/IVuplZJlzQzcwykWWXy/jx1b/WFVdcUVISM7PW8RW6mVkmXNDNzDLhgm5mlglFRP2lmrUxqRt4GtgXeK5lG26MMzXGmRrXjrmcqTHtlumNEdFZb6GWFvSdG5W6ImJhyzc8BGdqjDM1rh1zOVNj2jFTI9zlYmaWCRd0M7NMlFXQLy9pu0NxpsY4U+PaMZczNaYdM9VVSh+6mZk1n7tczMwy0dKCLmmxpCckPSnp3FZuuybHlZK2SlpdMW2WpNslrU0/Z7Y40zxJd0paI+kxSZ8uO5ekyZLuk/RwynRBmn6wpHtTpuslTWxVpopsHZIelLSiHTJJekrSo5IektSVppV9Ts2QdKOkH6fz6rg2yPSmtI/6hxclfaYNcv1ZOsdXS7ounfuln+fD1bKCLqkD+BrwXuAIYImkI1q1/RpXAYtrpp0LrIyIw4CVqd1KPcDZEXE4sAj4eNo/ZeZ6FTgpIo4CFgCLJS0CLgYuTZleAM5qYaZ+nwbWVLTbIdOJEbGg4u1uZZ9T/wD8R0S8GTiKYn+Vmikinkj7aAHwduAV4Htl5pI0B/gUsDAijgQ6gA/RHufU8ERESwbgOOAHFe3zgPNatf0B8swHVle0nwBmp/HZwBNlZUsZbgVOaZdcwFTgAeBYig9cjB/ouLYoy1yKP/qTgBWA2iDTU8C+NdNKO3bAPsB60utk7ZBpgIynAv9ddi5gDrARmEVxw8IVwGlln1MjGVrZ5dK/0/ptStPaxf4RsRkg/dyvrCCS5gNHA/eWnSt1bTwEbAVuB34KbIuInrRIGcfxMuAcoP/bwN/QBpkC+KGkVZKWpWllHrtDgG7gm6lr6gpJe5WcqdaHgOvSeGm5IuIZ4BJgA7AZ+AWwivLPqWFrZUHXANP8FpsakvYGbgI+ExEvlp0nInqjeHo8FzgGOHygxVqVR9LvAFsjYlXl5AEWbfW5dXxEvI2iS/Hjkt7d4u3XGg+8Dfh6RBwNvEzru3wGlfqj3w98tw2yzATOAA4GDgT2ojiOtdq+XrWyoG8C5lW05wLPtnD79WyRNBsg/dza6gCSJlAU82sj4uZ2yQUQEduAuyj692dI6r/pfKuP4/HA+yU9BXyHotvlspIzERHPpp9bKfqEj6HcY7cJ2BQR96b2jRQFvi3OJ4qC+UBEbEntMnOdDKyPiO6IeA24GXgnJZ9TI9HKgn4/cFh65XgixdOt21q4/XpuA5am8aUUfdgtI0nAcmBNRHy5HXJJ6pQ0I41PoTjx1wB3Ah8oI1NEnBcRcyNiPsU5dEdEfLjMTJL2kjStf5yib3g1JR67iPgZsFHSm9Kk9wCPl5mpxhJ2dbdAubk2AIskTU1/h/37qrRzasRa2WEPnA78hKIf9s/LeuGA4kTaDLxGcSVzFkU/7Epgbfo5q8WZ3kXxlO4R4KE0nF5mLuCtwIMp02rgi2n6IcB9wJMUT5knlXQcTwBWlJ0pbfvhNDzWf263wTm1AOhKx+8WYGbZmVKuqcDPgekV08reVxcAP07n+TXApHY5z4cz+JOiZmaZ8CdFzcwy4YJuZpYJF3Qzs0y4oJuZZcIF3cwsEy7oZmaZcEE3M8uEC7qZWSb+H5QO+l4z9jx/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first resize\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                   T.Resize(40, interpolation=Image.CUBIC),\n",
    "                   T.ToTensor()])\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return_val = int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "    #print(\"get_cart_location: \", return_val)\n",
    "    #print(\"scale\", scale)\n",
    "    #print(\"screen_width\", screen_width)\n",
    "    return return_val\n",
    "\n",
    "def get_screen():\n",
    "    # return screen requested by gym is 400*600*3\n",
    "    \n",
    "    # first change to torch order channel*height*width\n",
    "    screen = env.render(mode=\"rgb_array\").transpose((2, 0 ,1))\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    # in cartpole-v0, we can cut off the top and bottom of the screen\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height*0.8)]\n",
    "    # now cut width\n",
    "    view_width = int(screen_width*0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width//2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width)//2:\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width//2,\n",
    "                           cart_location + view_width//2)\n",
    "    screen = screen[:, :, slice_range]\n",
    "    \n",
    "    # conver to float, rescale to 0-1, to tensor, to device\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    screen = resize(screen).unsqueeze(0).to(device)\n",
    "    \n",
    "    return screen\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1,2,0).numpy(),\n",
    "          interpolation='none')\n",
    "plt.title('example extracted screen')\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T19:09:54.964708Z",
     "start_time": "2019-06-12T19:09:54.959778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02870912,  0.00937056, -0.03623778,  0.0379286 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
